<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Creating an AI chat in Python that can search documents (RAG) - Nicolas Leao</title>
<link href="../styles.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-dark.min.css" rel="stylesheet"/>
<script defer="" src="../index.js"></script>
</head>
<body>
<header>
<div class="site-logo">
<a href="/">
<img alt="Nicolas Leao" src="../assets/logo-small-3.png"/>
</a>
</div>
<button aria-label="Toggle menu" class="menu-toggle">
<span class="hamburger"></span>
</button>
<nav>
<a href="/">home</a>
<a href="https://github.com/nicolasleao" rel="noopener noreferrer" target="_blank">about</a>
<a href="https://www.youtube.com/@nicolasleao-tech" rel="noopener noreferrer" target="_blank">youtube</a>
<a href="https://cognition.digital" rel="noopener noreferrer" target="_blank">community</a>
</nav>
</header>
<main>
<article>
<div class="post-header">
<h1 class="post-title">Creating an AI chat in Python that can search documents (RAG)</h1>
<div class="post-date">Apr 03, 2025</div>
</div>
<div class="post-content"><h1 id="creating-an-ai-chat-in-python-that-can-search-documents-rag">Creating an AI chat in Python that can search documents (RAG)</h1>
<p>In this article we're going to create our own AI chat, capable of indexing markdown and PDF documents and answering questions about said documents completely for free and hosted locally using Python3, LlamaIndex, Qdrant and Ollama.</p>
<h2 id="core-concepts">Core Concepts</h2>
<p>Retrieval-Augmented Generation (RAG):
RAG is a way of optimizing a large language model using your own knowledge base. It consists of storing your documents in a database, matching them against the user's query, and providing the document as context to the LLM to generate an informed answer. It is often used in search engines and chat applications.</p>
<p>Diagram explaining how retrieval augmented generation works</p>
<h3 id="vectors">Vectors:</h3>
<p>Vectors are simply an array of numbers, but there are AI models that are used to transform text into multidimensional vectors, making it easy to group chunks of text by meaning, topic, semantic and gramatical similarities. These models are are called embedding models, and we will be using a model called bge-small-en-v1.5 which is available at HuggingFace</p>
<p>Diagram demonstrating how document embedding works</p>
<h3 id="vector-databases">Vector Databases:</h3>
<p>Vector databases are just like other DBMS like PostgreSQL or MongoDB, but they are optimized for storing and querying vectors, making them very useful for our RAG. We will use the Qdrant Vector Database because it is open-source and very easy to set-up.</p>
<h2 id="step-1-project-setup">Step 1: Project Setup</h2>
<p>For this tutorial you will need to have python and docker installed in your system.</p>
<p>First, create a folder for your project, and create a python virtual environment to install the dependencies.</p>
<div class="codehilite"><pre><span></span><code>python<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>venv
</code></pre></div>
<p>This will create the venv/ directory, you can activate the virtual environment using</p>
<div class="codehilite"><pre><span></span><code><span class="nb">source</span><span class="w"> </span>venv/bin/activate
</code></pre></div>
<p>Then, we need to install the dependencies for this project using PIP</p>
<div class="codehilite"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>qdrant_client<span class="w"> </span>llama-index<span class="w"> </span>ollama<span class="w"> </span>torch<span class="w"> </span>transformers
</code></pre></div>
<p>Next, we need to create a docker-compose.yml file to setup our Qdrant container.</p>
<blockquote>
<p>docker-compose.yml</p>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="nt">services</span><span class="p">:</span>
<span class="w">  </span><span class="nt">qdrant</span><span class="p">:</span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">qdrant/qdrant:latest</span>
<span class="w">    </span><span class="nt">restart</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">always</span>
<span class="w">    </span><span class="nt">container_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">polyrag-qdrant</span>
<span class="w">    </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6333:6333</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6334:6334</span>
<span class="w">    </span><span class="nt">expose</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6333</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6334</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6335</span>
<span class="w">    </span><span class="nt">configs</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">source</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">qdrant_config</span>
<span class="w">        </span><span class="nt">target</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/qdrant/config/production.yaml</span>
<span class="w">    </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">./qdrant_data:/qdrant/storage</span>

<span class="nt">configs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">qdrant_config</span><span class="p">:</span>
<span class="w">    </span><span class="nt">content</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">      </span><span class="no">log_level: INFO</span>
</code></pre></div>
<p>Running docker compose up -d should pull, build and start your qdrant instance</p>
<p>That should allow you to access Qdrant's WEB UI on port 6333 to manage your collections</p>
<p>http://localhost:6333/dashboard</p>
<p>Then, you can click on the console tab in the left to run HTTP requests directly in your browser to create, fetch and manage collections easily.</p>
<p>Let's create a collection to store our indexed documents, I'm calling my collection "polyrag_documents".</p>
<p>The "size" parameter depends on the Embedding Model that you're going to use, the model I'm using is bge-small-en-v1.5 which produces a vector with the dimension 384, that's where this number comes from.</p>
<div class="codehilite"><pre><span></span><code><span class="err">PUT</span><span class="w"> </span><span class="err">/collec</span><span class="kc">t</span><span class="err">io</span><span class="kc">ns</span><span class="err">/polyrag_docume</span><span class="kc">nts</span>
<span class="p">{</span>
<span class="w">    </span><span class="nt">"vectors"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">"size"</span><span class="p">:</span><span class="w"> </span><span class="mi">384</span><span class="p">,</span>
<span class="w">      </span><span class="nt">"distance"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Cosine"</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>Now we can start working on our indexer. I want it to index all the documents inside the data/ folder that I'll create. For now I'll just create a demo document called document1.md with a review of the movie Fight Club</p>
<blockquote>
<p>/data/document1.md</p>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="gh"># Fight Club Review</span>

Pitt dominates the screen every second he's on it, and it simultaneously represents his weirdest, funniest and most charismatic role of his career.
</code></pre></div>
<p>I'll create a file called <code>indexer.py</code> to run all the embedding and indexing into our Qdrant server.</p>
<p>The first step is importing Ollama, the Qdrant client, and some tools from LlamaIndex to create a connection to our database.</p>
<p>LlamaIndex is a library that will greatly help us speed up the process of setting up our RAG, it comes with a variety of tools to help us ingest and query our documents.</p>
<blockquote>
<p>indexer.py</p>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># Import modules</span>
<span class="kn">from</span> <span class="nn">llama_index.llms.ollama</span> <span class="kn">import</span> <span class="n">Ollama</span>
<span class="kn">import</span> <span class="nn">qdrant_client</span>
<span class="kn">from</span> <span class="nn">llama_index.core</span> <span class="kn">import</span> <span class="n">SimpleDirectoryReader</span><span class="p">,</span> <span class="n">StorageContext</span><span class="p">,</span> <span class="n">Settings</span>
<span class="kn">from</span> <span class="nn">llama_index.vector_stores.qdrant</span> <span class="kn">import</span> <span class="n">QdrantVectorStore</span>
<span class="kn">from</span> <span class="nn">llama_index.embeddings.huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbedding</span>
<span class="kn">from</span> <span class="nn">llama_index.core.node_parser</span> <span class="kn">import</span> <span class="n">SentenceSplitter</span>
<span class="kn">from</span> <span class="nn">llama_index.core.extractors</span> <span class="kn">import</span> <span class="n">TitleExtractor</span>
<span class="kn">from</span> <span class="nn">llama_index.core.ingestion</span> <span class="kn">import</span> <span class="n">IngestionPipeline</span>

<span class="c1"># Initialize Ollama and set LlamaIndex Settings</span>
<span class="n">Settings</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">Ollama</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"mistral"</span><span class="p">)</span>
<span class="n">Settings</span><span class="o">.</span><span class="n">embed_model</span> <span class="o">=</span> <span class="n">HuggingFaceEmbedding</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">"BAAI/bge-small-en-v1.5"</span><span class="p">)</span>

<span class="c1"># Create a Qdrant client and connect to our qdrant server and collection</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">qdrant_client</span><span class="o">.</span><span class="n">QdrantClient</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">"http://localhost:6333"</span><span class="p">)</span>
<span class="n">vector_store</span> <span class="o">=</span> <span class="n">QdrantVectorStore</span><span class="p">(</span><span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span> <span class="n">collection_name</span><span class="o">=</span><span class="s2">"polyrag_documents"</span><span class="p">)</span>
<span class="n">storage_context</span> <span class="o">=</span> <span class="n">StorageContext</span><span class="o">.</span><span class="n">from_defaults</span><span class="p">(</span><span class="n">vector_store</span><span class="o">=</span><span class="n">vector_store</span><span class="p">)</span>
</code></pre></div>
<p>Now let's create an ingestion pipeline to process our documents and add them to Qdrant.</p>
<p>You can tweak the settings for your use-case, I'm using chunk_size=1024 and chunk_overlap=64.</p>
<blockquote>
<p>indexer.py</p>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># Create the pipeline with transformations</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">IngestionPipeline</span><span class="p">(</span>
    <span class="n">transformations</span><span class="o">=</span><span class="p">[</span>
        <span class="n">SentenceSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">64</span><span class="p">),</span>
        <span class="n">TitleExtractor</span><span class="p">(),</span>
        <span class="n">HuggingFaceEmbedding</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">"BAAI/bge-small-en-v1.5"</span><span class="p">),</span>
    <span class="p">],</span>
    <span class="n">vector_store</span><span class="o">=</span><span class="n">vector_store</span>
<span class="p">)</span>

<span class="c1"># Load markdown and pdf documents</span>
<span class="n">reader</span> <span class="o">=</span> <span class="n">SimpleDirectoryReader</span><span class="p">(</span><span class="n">input_dir</span><span class="o">=</span><span class="s2">"./data"</span><span class="p">,</span> <span class="n">required_exts</span><span class="o">=</span><span class="p">[</span><span class="s2">".pdf"</span><span class="p">,</span> <span class="s2">".md"</span><span class="p">])</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="c1"># Run our ingestion pipeline</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">documents</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Document indexing completed."</span><span class="p">)</span>
</code></pre></div>
<p>We can now run python indexer.py to pull the embedding model from HuggingFace and index our files in the database.</p>
<p>Alright, so now we need a way to interact with the data we've indexed, so let's create another file called chat.py to create a conversation loop.</p>
<p>The first lines of this file are very similar to indexer.py as we will also need to connect to qdrant in the same way, however we don't need to import the ingestion pipeline as we will only be reading from the database in this file.</p>
<blockquote>
<p>chat.py</p>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># Import modules</span>
<span class="kn">from</span> <span class="nn">llama_index.llms.ollama</span> <span class="kn">import</span> <span class="n">Ollama</span>
<span class="kn">import</span> <span class="nn">qdrant_client</span>
<span class="kn">from</span> <span class="nn">llama_index.core</span> <span class="kn">import</span> <span class="n">VectorStoreIndex</span><span class="p">,</span> <span class="n">StorageContext</span><span class="p">,</span> <span class="n">Settings</span>
<span class="kn">from</span> <span class="nn">llama_index.vector_stores.qdrant</span> <span class="kn">import</span> <span class="n">QdrantVectorStore</span>
<span class="kn">from</span> <span class="nn">llama_index.embeddings.huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbedding</span>

<span class="c1"># Initialize Ollama and ServiceContext</span>
<span class="n">Settings</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">Ollama</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"mistral"</span><span class="p">)</span>
<span class="n">Settings</span><span class="o">.</span><span class="n">embed_model</span> <span class="o">=</span> <span class="n">HuggingFaceEmbedding</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">"BAAI/bge-small-en-v1.5"</span><span class="p">)</span>

<span class="c1"># Create Qdrant client and store</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">qdrant_client</span><span class="o">.</span><span class="n">QdrantClient</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">"http://localhost:6333"</span><span class="p">)</span>
<span class="n">vector_store</span> <span class="o">=</span> <span class="n">QdrantVectorStore</span><span class="p">(</span><span class="n">client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span> <span class="n">collection_name</span><span class="o">=</span><span class="s2">"polyrag_documents"</span><span class="p">)</span>
<span class="n">storage_context</span> <span class="o">=</span> <span class="n">StorageContext</span><span class="o">.</span><span class="n">from_defaults</span><span class="p">(</span><span class="n">vector_store</span><span class="o">=</span><span class="n">vector_store</span><span class="p">)</span>
</code></pre></div>
<p>Then, we can create a LlamaIndex query engine to interact with our database, and define a loop that asks for the user's input and sends it as a query to our index, and repeats until the user types in "bye".</p>
<blockquote>
<p>chat.py</p>
</blockquote>
<div class="codehilite"><pre><span></span><code><span class="c1"># Create VectorStoreIndex and query engine</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">VectorStoreIndex</span><span class="o">.</span><span class="n">from_vector_store</span><span class="p">(</span>
    <span class="n">vector_store</span><span class="o">=</span><span class="n">vector_store</span>
<span class="p">)</span>
<span class="n">query_engine</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">as_query_engine</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'PolyRAG started, ask any questions about your documents or type </span><span class="se">\'</span><span class="s1">bye</span><span class="se">\'</span><span class="s1"> to leave'</span><span class="p">)</span>
<span class="c1"># chat loop to perform a query and print the response</span>
<span class="n">running</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">while</span> <span class="n">running</span><span class="p">:</span>
    <span class="n">query</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s1">'$ you -&gt; '</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">query</span> <span class="o">==</span> <span class="s1">'quit'</span> <span class="ow">or</span> <span class="n">query</span> <span class="o">==</span> <span class="s1">'exit'</span> <span class="ow">or</span> <span class="n">query</span> <span class="o">==</span> <span class="s1">'bye'</span> <span class="ow">or</span> <span class="n">query</span> <span class="o">==</span> <span class="s1">'close'</span><span class="p">:</span>
        <span class="n">running</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">query_engine</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'$ polyrag -&gt; '</span><span class="p">,</span> <span class="n">response</span><span class="p">)</span>
</code></pre></div>
<p>So now we can run python chat.py to interact with our LLM enhanced with our knowledge base (everything in the /data/ folder).</p>
<div class="codehilite"><pre><span></span><code>python<span class="w"> </span>chat.py
PolyRAG<span class="w"> </span>started,<span class="w"> </span>ask<span class="w"> </span>any<span class="w"> </span>questions<span class="w"> </span>about<span class="w"> </span>your<span class="w"> </span>documents<span class="w"> </span>or<span class="w"> </span><span class="nb">type</span><span class="w"> </span><span class="s1">'bye'</span><span class="w"> </span>to<span class="w"> </span>leave
$<span class="w"> </span>you<span class="w"> </span>-&gt;<span class="w"> </span>what<span class="w"> </span>does<span class="w"> </span>the<span class="w"> </span>author<span class="w"> </span>think<span class="w"> </span>about<span class="w"> </span>fight<span class="w"> </span>club
$<span class="w"> </span>polyrag<span class="w"> </span>-&gt;
<span class="w">  </span>The<span class="w"> </span>author<span class="w"> </span>seems<span class="w"> </span>to<span class="w"> </span>hold<span class="w"> </span>a<span class="w"> </span>positive<span class="w"> </span>opinion<span class="w"> </span>towards<span class="w"> </span>Fight<span class="w"> </span>Club,<span class="w"> </span>as<span class="w"> </span>indicated<span class="w"> </span>by<span class="w"> </span>
<span class="w">  </span>their<span class="w"> </span>statement<span class="w"> </span>that<span class="w"> </span>Brad<span class="w"> </span>Pitt<span class="w"> </span>dominates<span class="w"> </span>the<span class="w"> </span>screen<span class="w"> </span>every<span class="w"> </span>second<span class="w"> </span>he<span class="s1">'s on it and </span>
<span class="s1">  that this role represents Pitt'</span>s<span class="w"> </span>weirdest,<span class="w"> </span>funniest,<span class="w"> </span>and<span class="w"> </span>most<span class="w"> </span>charismatic<span class="w"> </span>
<span class="w">  </span>performance<span class="w"> </span>of<span class="w"> </span>his<span class="w"> </span>career.<span class="w"> </span>This<span class="w"> </span>suggests<span class="w"> </span>that<span class="w"> </span>they<span class="w"> </span>found<span class="w"> </span>the<span class="w"> </span>movie<span class="w"> </span>engaging<span class="w"> </span>and<span class="w"> </span>
<span class="w">  </span>enjoyed<span class="w"> </span>Pitt<span class="err">'</span>s<span class="w"> </span>performance<span class="w"> </span><span class="k">in</span><span class="w"> </span>it.
$<span class="w"> </span>you<span class="w"> </span>-&gt;<span class="w"> </span>
</code></pre></div>
<p>Note that everytime you need to add a new file to the knowledge base, you must copy it to the /data/ folder and run python indexer.py again.</p>
<p>That's it! Your simple RAG application is ready. Feel free to extend it's funcionality however you see fit.</p>
<p>This article covers the basis of how I created PolyRAG, a python server based on FastAPI to index and query files just like we did here. You can check the source code for PolyRAG here.</p></div>
</article>
</main>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
<script>
        document.addEventListener('DOMContentLoaded', () => {
            // Apply syntax highlighting to all code blocks
            document.querySelectorAll('pre code').forEach((block) => {
                hljs.highlightElement(block);
            });
        });
    </script>
</body>
</html> 